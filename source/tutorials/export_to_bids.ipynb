{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting to BIDS Formats\n",
    "\n",
    "## What is BIDS and Why Use It?\n",
    "\n",
    "The [Brain Imaging Data Structure (BIDS)](https://bids.neuroimaging.io/index.html) is a comprehensive framework designed to systematically organize and share diverse types of data, including behavioral, physiological, and neuroimaging information. Converting datasets into BIDS format is a widely adopted methodology, particularly in the process of curating datasets that adhere to the principles of FAIR (Findable, Accessible, Interoperable, Reusable).\n",
    "\n",
    "**Key benefits of using BIDS:**\n",
    "- **Standardization**: Consistent naming conventions and directory structures across datasets\n",
    "- **Interoperability**: Enables automated analysis pipelines and data sharing\n",
    "- **Reproducibility**: Comprehensive metadata ensures experiments can be understood and replicated\n",
    "- **Community adoption**: Widely accepted format in neuroscience research\n",
    "\n",
    "The general framework of BIDS is described in the following publication:\n",
    "\n",
    "> <cite>Gorgolewski, K., Auer, T., Calhoun, V. et al. The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. Sci Data 3, 160044 (2016). https://doi.org/10.1038/sdata.2016.44<cite>\n",
    "\n",
    "## BIDS Extensions for Neon Data\n",
    "\n",
    "For datasets from Pupil Labs Neon eye-tracker, we utilize two BIDS extensions:\n",
    "\n",
    "1. **Motion-BIDS (BEP029)**: Organizes motion data including acceleration, angular velocity (gyroscope), and orientation from the IMU sensor\n",
    "2. **Eye-Tracking-BIDS (BEP020)**: Organizes gaze position, pupil size/diameter data, and eye-tracking events (fixations, saccades, blinks)\n",
    "\n",
    "In this tutorial, we demonstrate how to export Neon recordings to these BIDS formats using PyNeon's `export_motion_bids()` and `export_eye_bids()` methods. These functions handle all file naming, metadata generation, and formatting requirements automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from seedir import seedir\n",
    "from pyneon import Dataset, get_sample_data\n",
    "\n",
    "# Load sample data\n",
    "dataset = Dataset(get_sample_data(\"markers\", format=\"cloud\"))\n",
    "rec = dataset.recordings[0]\n",
    "\n",
    "print(f\"Recording: {rec.info['recording_name']}\")\n",
    "print(f\"Wearer: {rec.info['wearer_name']}\")\n",
    "print(f\"Duration: {rec.duration / 1e9:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to Motion-BIDS\n",
    "\n",
    "The Motion-BIDS specification provides a standardized way to organize motion sensor data from devices like IMUs (Inertial Measurement Units):\n",
    "\n",
    "> <cite>Jeung, S., Cockx, H., Appelhoff, S. et al. Motion-BIDS: an extension to the brain imaging data structure to organize motion data for reproducible research. Sci Data 11, 716 (2024). https://doi.org/10.1038/s41597-024-03559-8<cite>\n",
    "\n",
    "### Understanding the BIDS Prefix\n",
    "\n",
    "The `export_motion_bids()` method requires a **prefix** string that specifies the experimental context. The prefix follows this standardized format (fields in brackets are optional):\n",
    "\n",
    "```text\n",
    "sub-<label>[_ses-<label>]_task-<label>_tracksys-<label>[_acq-<label>][_run-<index>]\n",
    "```\n",
    "\n",
    "**Required fields:**\n",
    "- `sub-<label>`: Subject/participant identifier (e.g., `sub-01`, `sub-Alice`)\n",
    "- `task-<label>`: Name of the experimental task (e.g., `task-Navigation`, `task-Reading`)\n",
    "- `tracksys-<label>`: Tracking system used (for Neon IMU: `tracksys-NeonIMU`)\n",
    "\n",
    "**Optional fields:**\n",
    "- `ses-<label>`: Session identifier for multi-session experiments\n",
    "- `acq-<label>`: Acquisition parameters or protocol\n",
    "- `run-<index>`: Run number for repeated acquisitions\n",
    "\n",
    "### Adding Custom Metadata\n",
    "\n",
    "You can include additional experiment-specific metadata by passing a dictionary to the `extra_metadata` argument. This information will be saved in the JSON metadata file and is crucial for documenting your experimental setup.\n",
    "\n",
    "Let's export the motion data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion/\n",
      "├─sub-1_task-LabMuse_tracksys-NeonIMU_run-1_channels.json\n",
      "├─sub-1_task-LabMuse_tracksys-NeonIMU_run-1_channels.tsv\n",
      "├─sub-1_task-LabMuse_tracksys-NeonIMU_run-1_motion.json\n",
      "├─sub-1_task-LabMuse_tracksys-NeonIMU_run-1_motion.tsv\n",
      "├─sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physio.json\n",
      "├─sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physio.tsv.gz\n",
      "├─sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physioevents.json\n",
      "└─sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physioevents.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "# Create a BIDS directory\n",
    "motion_dir = Path(\"export\") / \"BIDS\" / \"sub-1\" / \"motion\"\n",
    "\n",
    "# Export the motion data to BIDS format\n",
    "prefix = \"sub-1_task-LabMuse_tracksys-NeonIMU_run-1\"\n",
    "extra_metadata = {\n",
    "    \"TaskName\": \"LabMuse\",\n",
    "    \"InstitutionName\": \"Streeling University\",\n",
    "    \"InstitutionAddress\": \"Trantor, Galactic Empire\",\n",
    "    \"InstitutionalDepartmentName\": \"Department of Psychohistory\",\n",
    "}\n",
    "\n",
    "rec.export_motion_bids(motion_dir, prefix=prefix, extra_metadata=extra_metadata)\n",
    "\n",
    "seedir(motion_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Motion-BIDS File Structure\n",
    "\n",
    "The export creates four files that work together to fully describe the IMU data:\n",
    "\n",
    "1. **`_motion.tsv`**: Tab-separated file containing the raw IMU time-series data (no header)\n",
    "2. **`_motion.json`**: Metadata describing the recording setup, device, and data characteristics\n",
    "3. **`_channels.tsv`**: Information about each data channel (type, units, sampling rate)\n",
    "4. **`_channels.json`**: Coordinate system information for the motion data\n",
    "\n",
    "Additionally, a **`_scans.tsv`** file is created in the parent directory to log all acquisitions for the subject/session.\n",
    "\n",
    "Let's examine each file in detail.\n",
    "\n",
    "#### 1. Motion Time-Series Data (`_motion.tsv`)\n",
    "\n",
    "This file contains the continuous IMU measurements. Each row is a sample, and each column is a sensor channel (13 total: 3 gyroscope + 3 accelerometer + 7 orientation quaternion):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion data shape: (25105, 13)\n",
      "Channels: 3 gyroscope + 3 accelerometer + 7 orientation quaternion = 13 total\n",
      "\n",
      "First few samples:\n",
      "          0         1          2         3         4         5         6   \\\n",
      "0 -51.717758 -2.668381  60.407639 -0.239258 -0.118164  0.899902  7.143754   \n",
      "1 -58.860779 -2.059937  67.914963 -0.230957 -0.143066  0.915527  7.216643   \n",
      "2 -65.940857 -1.632690  75.727463 -0.208008 -0.144043  0.927246  7.297498   \n",
      "3 -76.131821  0.989914  89.948654 -0.208008 -0.158691  0.926758  7.394377   \n",
      "4 -79.427719  3.431320  95.075607 -0.187012 -0.171875  0.937012  7.515937   \n",
      "\n",
      "         7           8         9         10        11        12  \n",
      "0 -3.179328 -106.205219  0.597605  0.033180  0.059532 -0.798889  \n",
      "1 -3.664375 -105.502129  0.602169  0.030758  0.063474 -0.795247  \n",
      "2 -4.196307 -104.722450  0.607190  0.028048  0.067771 -0.791164  \n",
      "3 -4.766157 -103.867197  0.612648  0.025139  0.072393 -0.786632  \n",
      "4 -5.370411 -102.939744  0.618502  0.022095  0.077353 -0.781654  \n",
      "\n",
      "Data summary:\n",
      "Total samples: 25105\n"
     ]
    }
   ],
   "source": [
    "physio_tsv_path = motion_dir / f\"{prefix}_motion.tsv\"\n",
    "physio_df = pd.read_csv(physio_tsv_path, sep=\"\\t\", header=None)\n",
    "print(f\"Motion data shape: {physio_df.shape}\")\n",
    "print(physio_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Motion Metadata (`_motion.json`)\n",
    "\n",
    "This file contains crucial metadata about the recording setup and data characteristics. Note how our custom metadata (TaskName, InstitutionName, etc.) has been included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TaskName\": \"LabMuse\",\n",
      "    \"TaskDescription\": \"\",\n",
      "    \"Instructions\": \"\",\n",
      "    \"DeviceSerialNumber\": \"114837\",\n",
      "    \"Manufacturer\": \"TDK InvenSense & Pupil Labs\",\n",
      "    \"ManufacturersModelName\": \"ICM-20948\",\n",
      "    \"SoftwareVersions\": \"App version: 2.9.26-prod; Pipeline version: 2.8.0\",\n",
      "    \"InstitutionName\": \"Streeling University\",\n",
      "    \"InstitutionAddress\": \"Trantor, Galactic Empire\",\n",
      "    \"InstitutionalDepartmentName\": \"Department of Psychohistory\",\n",
      "    \"SamplingFrequency\": 110,\n",
      "    \"ACCELChannelCount\": 3,\n",
      "    \"GYROChannelCount\": 3,\n",
      "    \"MissingValues\": \"n/a\",\n",
      "    \"MotionChannelCount\": 13,\n",
      "    \"ORNTChannelCount\": 7,\n",
      "    \"SubjectArtefactDescription\": \"\",\n",
      "    \"TrackedPointsCount\": 0,\n",
      "    \"TrackingSystemName\": \"IMU included in Neon\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "motion_json = motion_dir / f\"{prefix}_motion.json\"\n",
    "with open(motion_json, \"r\") as f:\n",
    "    motion_metadata = json.load(f)\n",
    "print(json.dumps(motion_metadata, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Channel Information (`_channels.tsv`)\n",
    "\n",
    "This file provides detailed information about each channel in the motion data, including the sensor type, spatial component (x/y/z), units, and sampling frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              name component   type tracked_point      units  \\\n",
      "0           gyro x         x   GYRO          Head      deg/s   \n",
      "1           gyro y         y   GYRO          Head      deg/s   \n",
      "2           gyro z         z   GYRO          Head      deg/s   \n",
      "3   acceleration x         x  ACCEL          Head          g   \n",
      "4   acceleration y         y  ACCEL          Head          g   \n",
      "5   acceleration z         z  ACCEL          Head          g   \n",
      "6             roll         x   ORNT          Head        deg   \n",
      "7            pitch         y   ORNT          Head        deg   \n",
      "8              yaw         z   ORNT          Head        deg   \n",
      "9     quaternion w         w   ORNT          Head  arbitrary   \n",
      "10    quaternion x         x   ORNT          Head  arbitrary   \n",
      "11    quaternion y         y   ORNT          Head  arbitrary   \n",
      "12    quaternion z         z   ORNT          Head  arbitrary   \n",
      "\n",
      "    sampling_frequency  \n",
      "0                  103  \n",
      "1                  103  \n",
      "2                  103  \n",
      "3                  103  \n",
      "4                  103  \n",
      "5                  103  \n",
      "6                  103  \n",
      "7                  103  \n",
      "8                  103  \n",
      "9                  103  \n",
      "10                 103  \n",
      "11                 103  \n",
      "12                 103  \n"
     ]
    }
   ],
   "source": [
    "channels_tsv_path = motion_dir / f\"{prefix}_channels.tsv\"\n",
    "channels_df = pd.read_csv(channels_tsv_path, sep=\"\\t\")\n",
    "print(channels_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sensor types in Neon IMU:**\n",
    "- **GYRO**: Angular velocity (rotation rate) in degrees/second\n",
    "- **ACCEL**: Linear acceleration in g-force units\n",
    "- **ORNT**: Orientation quaternion (w, x, y, z) in arbitrary units\n",
    "\n",
    "#### 4. Coordinate System (`_channels.json`)\n",
    "\n",
    "This file defines the reference frame for interpreting the motion data. For Neon, the global reference frame is defined by the IMU axes (X=right, Y=anterior, Z=superior):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"reference_frame\": {\n",
      "        \"Levels\": {\n",
      "            \"global\": {\n",
      "                \"SpatialAxes\": \"RAS\",\n",
      "                \"RotationOrder\": \"ZXY\",\n",
      "                \"RotationRule\": \"right-hand\",\n",
      "                \"Description\": \"This global reference frame is defined by the IMU axes: X right, Y anterior, Z superior. The scene camera frame differs from this frame by a 102-degree rotation around the X-axis. All motion data are expressed relative to the IMU frame for consistency.\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "channels_json_path = motion_dir / f\"{prefix}_channels.json\"\n",
    "with open(channels_json_path, \"r\") as f:\n",
    "    channels_metadata = json.load(f)\n",
    "print(json.dumps(channels_metadata, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to Eye-Tracking-BIDS\n",
    "\n",
    "The Eye-Tracking-BIDS specification standardizes how gaze position, pupil data, and eye-tracking events should be organized:\n",
    "\n",
    "> <cite>Szinte, M., Bach, D. R., Draschkow, D., Esteban, O., Gagl, B., Gau, R., Gregorova, K., Halchenko, Y. O., Huberty, S., Kling, S. M., Kulkarni, S., Maintainers, T. B., Markiewicz, C. J., Mikkelsen, M., Oostenveld, R., & Pfarr, J.-K. (2026). Eye-Tracking-BIDS: The Brain Imaging Data Structure extended to gaze position and pupil data. bioRxiv. https://doi.org/10.64898/2026.02.03.703514<cite>\n",
    "\n",
    "### Export Configuration\n",
    "\n",
    "The `export_eye_bids()` method has similar arguments to `export_motion_bids()`:\n",
    "- **output_dir**: Directory where files will be saved\n",
    "- **prefix**: BIDS naming prefix (must include `sub-<label>` at minimum)\n",
    "- **extra_metadata**: Optional dictionary of additional metadata\n",
    "\n",
    "**Important**: Eye-tracking data is considered physiology data and can be placed together with most modalities of data. When exporting eye-tracking data, use the **matching prefix** as the modality data to link them together as part of the same recording session. In this example, we export to the same `motion` directory, but you could also use separate modality directories (e.g., for combined EEG+eye-tracking studies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Duplicated indices found and removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qian.chu\\Documents\\GitHub\\PyNeon\\pyneon\\preprocess\\preprocess.py:67: UserWarning: 23 out of 48219 requested timestamps are outside the data time range and will have empty data.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion/\n",
      "├─sub-1_task-LabMuse_tracksys-NeonIMU_run-1_channels.json\n",
      "├─sub-1_task-LabMuse_tracksys-NeonIMU_run-1_channels.tsv\n",
      "├─sub-1_task-LabMuse_tracksys-NeonIMU_run-1_motion.json\n",
      "├─sub-1_task-LabMuse_tracksys-NeonIMU_run-1_motion.tsv\n",
      "├─sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physio.json\n",
      "├─sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physio.tsv.gz\n",
      "├─sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physioevents.json\n",
      "└─sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physioevents.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "rec.export_eye_bids(motion_dir, prefix=prefix)\n",
    "seedir(motion_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Eye-Tracking-BIDS File Structure\n",
    "\n",
    "Eye-Tracking-BIDS creates four main files:\n",
    "\n",
    "1. **`_physio.tsv.gz`**: Compressed time-series data for gaze and pupil measurements\n",
    "2. **`_physio.json`**: Metadata describing the eye-tracking setup and data columns\n",
    "3. **`_physioevents.tsv.gz`**: Event data (fixations, saccades, blinks, custom messages)\n",
    "4. **`_physioevents.json`**: Metadata for the events file\n",
    "\n",
    "#### 1. Physiological Time-Series Data (`_physio.tsv.gz`)\n",
    "\n",
    "This compressed file contains continuous gaze and pupil data with 5 columns:\n",
    "- **timestamp**: Time in nanoseconds\n",
    "- **x_coordinate**: Horizontal gaze position in pixels\n",
    "- **y_coordinate**: Vertical gaze position in pixels\n",
    "- **left_pupil_diameter**: Left pupil diameter in millimeters\n",
    "- **right_pupil_diameter**: Right pupil diameter in millimeters\n",
    "\n",
    "Let's inspect the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eye-tracking data shape: (48219, 5)\n",
      "                     0        1         2       3       4\n",
      "0  1758493516475391023  535.360  1016.989  4.6517  4.5655\n",
      "1  1758493516480394023  519.555  1033.236  4.0452  4.5209\n",
      "2  1758493516485390023  521.541  1045.861  3.4693  4.5801\n",
      "3  1758493516490390023  503.323  1049.000  4.1283  4.7350\n",
      "4  1758493516495406023  498.538  1065.085  4.5646  4.8635\n"
     ]
    }
   ],
   "source": [
    "physio_tsv_path = motion_dir / f\"{prefix}_physio.tsv.gz\"\n",
    "physio_df = pd.read_csv(physio_tsv_path, sep=\"\\t\", compression=\"gzip\", header=None)\n",
    "print(f\"Eye-tracking data shape: {physio_df.shape}\")\n",
    "print(physio_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Physiological Data Metadata (`_physio.json`)\n",
    "\n",
    "This file provides comprehensive metadata about the eye-tracking data, including column definitions, sampling frequency, and device information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"SamplingFrequency\": 199.71919360432685,\n",
      "    \"StartTime\": 0,\n",
      "    \"Columns\": [\n",
      "        \"timestamp\",\n",
      "        \"x_coordinate\",\n",
      "        \"y_coordinate\",\n",
      "        \"left_pupil_diameter\",\n",
      "        \"right_pupil_diameter\"\n",
      "    ],\n",
      "    \"DeviceSerialNumber\": \"114837\",\n",
      "    \"Manufacturer\": \"Pupil Labs\",\n",
      "    \"ManufacturersModelName\": \"Neon\",\n",
      "    \"SoftwareVersions\": \"App version: 2.9.26-prod; Pipeline version: 2.8.0\",\n",
      "    \"PhysioType\": \"eyetrack\",\n",
      "    \"EnvironmentCoorinates\": \"top-left\",\n",
      "    \"RecordedEye\": \"cyclopean\",\n",
      "    \"SampleCoordinateSystem\": \"gaze-in-world\",\n",
      "    \"EyeTrackingMethod\": \"real-time neural network\",\n",
      "    \"timestamp\": {\n",
      "        \"Description\": \"UTC timestamp in nanoseconds of the sample\",\n",
      "        \"Units\": \"ns\"\n",
      "    },\n",
      "    \"x_coordinate\": {\n",
      "        \"Description\": \"X-coordinate of the mapped gaze point in world camera pixel coordinates.\",\n",
      "        \"Units\": \"pixel\"\n",
      "    },\n",
      "    \"y_coordinate\": {\n",
      "        \"Description\": \"Y-coordinate of the mapped gaze point in world camera pixel coordinates.\",\n",
      "        \"Units\": \"pixel\"\n",
      "    },\n",
      "    \"pupil_size_left\": {\n",
      "        \"Description\": \"Physical diameter of the pupil of the left eye\",\n",
      "        \"Units\": \"mm\"\n",
      "    },\n",
      "    \"pupil_size_right\": {\n",
      "        \"Description\": \"Physical diameter of the pupil of the right eye\",\n",
      "        \"Units\": \"mm\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "physio_json = motion_dir / f\"{prefix}_physio.json\"\n",
    "with open(physio_json, \"r\") as f:\n",
    "    physio_metadata = json.load(f)\n",
    "print(json.dumps(physio_metadata, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Eye-Tracking Events (`_physioevents.tsv.gz`)\n",
    "\n",
    "This file contains all detected eye-tracking events and custom messages. Each row represents one event with columns:\n",
    "- **onset**: Event start time in nanoseconds\n",
    "- **duration**: Event duration in seconds (for fixations, saccades, blinks)\n",
    "- **trial_type**: Type of event (`fixation`, `saccade`, `blink`)\n",
    "- **message**: Custom event messages/markers (when applicable)\n",
    "\n",
    "PyNeon automatically exports all available events from the recording, including algorithmically detected events (fixations, saccades, blinks) and user-defined messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eye-tracking data shape: (1100, 4)\n",
      "   1758493514096000000    n/a     n/a.1 recording.begin\n",
      "0  1758493516475391023  0.105   saccade             NaN\n",
      "1  1758493516580513023  0.535  fixation             NaN\n",
      "2  1758493517116002023  0.110   saccade             NaN\n",
      "3  1758493517226130023  0.135  fixation             NaN\n",
      "4  1758493517326259023  0.180     blink             NaN\n"
     ]
    }
   ],
   "source": [
    "physioevents_tsv_path = motion_dir / f\"{prefix}_physioevents.tsv.gz\"\n",
    "physioevents_df = pd.read_csv(physioevents_tsv_path, sep=\"\\t\", compression=\"gzip\")\n",
    "print(f\"Total events: {physioevents_df.shape[0]}\")\n",
    "print(physioevents_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Events Metadata (`_physioevents.json`)\n",
    "\n",
    "This file describes the structure and meaning of the events data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Columns\": [\n",
      "        \"onset\",\n",
      "        \"duration\",\n",
      "        \"trial_type\",\n",
      "        \"message\"\n",
      "    ],\n",
      "    \"Description\": \"Eye events and messages logged by Neon\",\n",
      "    \"OnsetSource\": \"timestamp\",\n",
      "    \"onset\": {\n",
      "        \"Description\": \"UTC timestamp in nanoseconds of the start of the event\",\n",
      "        \"Units\": \"ns\"\n",
      "    },\n",
      "    \"duration\": {\n",
      "        \"Description\": \"Event duration\",\n",
      "        \"Units\": \"s\"\n",
      "    },\n",
      "    \"trial_type\": {\n",
      "        \"Description\": \"Type of trial event\",\n",
      "        \"Levels\": {\n",
      "            \"fixation\": {\n",
      "                \"Description\": \"Fixation event\"\n",
      "            },\n",
      "            \"saccade\": {\n",
      "                \"Description\": \"Saccade event\"\n",
      "            },\n",
      "            \"blink\": {\n",
      "                \"Description\": \"Blink event\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "physioevents_json = motion_dir / f\"{prefix}_physioevents.json\"\n",
    "with open(physioevents_json, \"r\") as f:\n",
    "    physioevents_metadata = json.load(f)\n",
    "print(json.dumps(physioevents_metadata, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
