{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting to BIDS Formats\n",
    "\n",
    "\n",
    "The [Brain Imaging Data Structure (BIDS)](https://bids.neuroimaging.io/index.html) is a comprehensive framework designed to systematically organize and share diverse types of data, including behavioral, physiological, and neuroimaging information. Converting datasets into BIDS format is a widely adopted methodology, particularly in the process of curating datasets that adhere to the principles of FAIR (Findable, Accessible, Interoperable, Reusable).\n",
    "\n",
    "For datasets encompassing mobile eye-tracking data, it is essential to apply specific BIDS specifications tailored for such data. In this context, Motion-BIDS and Eye-Tracking-BIDS specifications are noteworthy. Motion-BIDS ([BEP029](https://github.com/bids-standard/bids-specification/pull/981)) has been successfully integrated into the official BIDS specification, demonstrating its readiness for use in organizing motion-related data. On the other hand, Eye-Tracking-BIDS ([BEP020](https://github.com/bids-standard/bids-specification/pull/1128)) is still undergoing development, reflecting ongoing efforts to provide a standardized format for eye-tracking data. You can find more information about these specifications in the following references:\n",
    "\n",
    "> <cite>Gorgolewski, K., Auer, T., Calhoun, V. et al. The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. Sci Data 3, 160044 (2016). https://doi.org/10.1038/sdata.2016.44<cite>\n",
    "\n",
    "> <cite>Jeung, S., Cockx, H., Appelhoff, S. et al. Motion-BIDS: an extension to the brain imaging data structure to organize motion data for reproducible research. Sci Data 11, 716 (2024). https://doi.org/10.1038/s41597-024-03559-8<cite>\n",
    "\n",
    "In the ensuing section, we will delve into the procedure for exporting data to Motion-BIDS. This will be accomplished using the `export_motion_bids` method available within PyNeon's `Recording` objects, offering a practical guide for researchers aiming to standardize their motion data in alignment with the BIDS framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pyneon import Dataset, get_sample_data\n",
    "\n",
    "dataset = Dataset(get_sample_data(\"markers\", format=\"cloud\"))\n",
    "rec = dataset.recordings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to Motion-BIDS\n",
    "\n",
    "To use `export_motion_bids` method, we need to specify the output directory where the BIDS dataset will be saved, and a string prefix to denote this session of data. The prefix follows the following format (Fields in [] are optional):\n",
    "\n",
    "```text\n",
    "sub-<label>[_ses-<label>]_task-<label>_tracksys-<label>[_acq-<label>][_run-<index>]\n",
    "```\n",
    "\n",
    "If you have any additional metadata that you would like to include, you can pass it as a dictionary to the `extra_metadata` argument. This metadata will be saved in the `dataset_description.json` file.\n",
    "\n",
    "Let's see what files will be exported to the BIDS dataset directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_channels.json\n",
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_channels.tsv\n",
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_motion.json\n",
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_motion.tsv\n",
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physio.json\n",
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physio.tsv.gz\n",
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physioevents.json\n",
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physioevents.tsv\n"
     ]
    }
   ],
   "source": [
    "# Create a BIDS directory\n",
    "motion_dir = Path(\"export\") / \"BIDS\" / \"sub-1\" / \"motion\"\n",
    "\n",
    "# Export the motion data to BIDS format\n",
    "prefix = \"sub-1_task-LabMuse_tracksys-NeonIMU_run-1\"\n",
    "extra_metadata = {\n",
    "    \"TaskName\": \"LabMuse\",\n",
    "    \"InstitutionName\": \"Streeling University\",\n",
    "    \"InstitutionAddress\": \"Trantor, Galactic Empire\",\n",
    "    \"InstitutionalDepartmentName\": \"Department of Psychohistory\",\n",
    "}\n",
    "\n",
    "rec.export_motion_bids(motion_dir, prefix=prefix, extra_metadata=extra_metadata)\n",
    "\n",
    "# Print all the conents of motion_dir\n",
    "for path in motion_dir.iterdir():\n",
    "    print(path.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of these files follow the Motion-BIDS specification at: https://bids-specification.readthedocs.io/en/stable/modality-specific-files/motion.html.\n",
    "\n",
    "For example, the `_motion.tsv` is a tab-separated values file that contains the (n_samples, n_channels) motion data without a header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion data shape: (25104, 13)\n",
      "   -51.717758  -2.668381  60.407639  -0.239258  -0.118164  0.899902  \\\n",
      "0  -58.860779  -2.059937  67.914963  -0.230957  -0.143066  0.915527   \n",
      "1  -65.940857  -1.632690  75.727463  -0.208008  -0.144043  0.927246   \n",
      "2  -76.131821   0.989914  89.948654  -0.208008  -0.158691  0.926758   \n",
      "3  -79.427719   3.431320  95.075607  -0.187012  -0.171875  0.937012   \n",
      "4  -81.747055   5.998611  99.592209  -0.150879  -0.189453  0.945801   \n",
      "\n",
      "   7.143753693332938  -3.1793275574051094  -106.20521917582651  \\\n",
      "0           7.216643            -3.664375          -105.502129   \n",
      "1           7.297498            -4.196307          -104.722450   \n",
      "2           7.394377            -4.766157          -103.867197   \n",
      "3           7.515937            -5.370411          -102.939744   \n",
      "4           7.657661            -5.993968          -101.953045   \n",
      "\n",
      "   0.5976047515869141  0.0331803187727928  0.0595318600535392  \\\n",
      "0            0.602169            0.030758            0.063474   \n",
      "1            0.607190            0.028048            0.067771   \n",
      "2            0.612648            0.025139            0.072393   \n",
      "3            0.618502            0.022095            0.077353   \n",
      "4            0.624662            0.018960            0.082515   \n",
      "\n",
      "   -0.7988889813423157  \n",
      "0            -0.795247  \n",
      "1            -0.791164  \n",
      "2            -0.786632  \n",
      "3            -0.781654  \n",
      "4            -0.776292  \n"
     ]
    }
   ],
   "source": [
    "physio_tsv_path = motion_dir / f\"{prefix}_motion.tsv\"\n",
    "physio_df = pd.read_csv(physio_tsv_path, sep=\"\\t\")\n",
    "print(f\"Motion data shape: {physio_df.shape}\")\n",
    "print(physio_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its metadata is stored in the `_motion.json` file, which contains (note the extra metadata we added):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TaskName\": \"LabMuse\",\n",
      "    \"TaskDescription\": \"\",\n",
      "    \"Instructions\": \"\",\n",
      "    \"DeviceSerialNumber\": \"114837\",\n",
      "    \"Manufacturer\": \"TDK InvenSense & Pupil Labs\",\n",
      "    \"ManufacturersModelName\": \"ICM-20948\",\n",
      "    \"SoftwareVersions\": \"App version: 2.9.26-prod; Pipeline version: 2.8.0\",\n",
      "    \"InstitutionName\": \"Streeling University\",\n",
      "    \"InstitutionAddress\": \"Trantor, Galactic Empire\",\n",
      "    \"InstitutionalDepartmentName\": \"Department of Psychohistory\",\n",
      "    \"SamplingFrequency\": 110,\n",
      "    \"ACCELChannelCount\": 3,\n",
      "    \"GYROChannelCount\": 3,\n",
      "    \"MissingValues\": \"n/a\",\n",
      "    \"MotionChannelCount\": 13,\n",
      "    \"ORNTChannelCount\": 7,\n",
      "    \"SubjectArtefactDescription\": \"\",\n",
      "    \"TrackedPointsCount\": 0,\n",
      "    \"TrackingSystemName\": \"IMU included in Neon\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "motion_json = motion_dir / f\"{prefix}_motion.json\"\n",
    "with open(motion_json, \"r\") as f:\n",
    "    motion_metadata = json.load(f)\n",
    "print(json.dumps(motion_metadata, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata for each channel (each column in the `_motion.tsv` file) is stored in `_channels.tsv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              name component   type tracked_point      units  \\\n",
      "0           gyro x         x   GYRO          Head      deg/s   \n",
      "1           gyro y         y   GYRO          Head      deg/s   \n",
      "2           gyro z         z   GYRO          Head      deg/s   \n",
      "3   acceleration x         x  ACCEL          Head          g   \n",
      "4   acceleration y         y  ACCEL          Head          g   \n",
      "5   acceleration z         z  ACCEL          Head          g   \n",
      "6             roll         x   ORNT          Head        deg   \n",
      "7            pitch         y   ORNT          Head        deg   \n",
      "8              yaw         z   ORNT          Head        deg   \n",
      "9     quaternion w         w   ORNT          Head  arbitrary   \n",
      "10    quaternion x         x   ORNT          Head  arbitrary   \n",
      "11    quaternion y         y   ORNT          Head  arbitrary   \n",
      "12    quaternion z         z   ORNT          Head  arbitrary   \n",
      "\n",
      "    sampling_frequency  \n",
      "0                  103  \n",
      "1                  103  \n",
      "2                  103  \n",
      "3                  103  \n",
      "4                  103  \n",
      "5                  103  \n",
      "6                  103  \n",
      "7                  103  \n",
      "8                  103  \n",
      "9                  103  \n",
      "10                 103  \n",
      "11                 103  \n",
      "12                 103  \n"
     ]
    }
   ],
   "source": [
    "channels_tsv_path = motion_dir / f\"{prefix}_channels.tsv\"\n",
    "channels_df = pd.read_csv(channels_tsv_path, sep=\"\\t\")\n",
    "print(channels_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `_channels.json` file contains the coordinate system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"reference_frame\": {\n",
      "        \"Levels\": {\n",
      "            \"global\": {\n",
      "                \"SpatialAxes\": \"RAS\",\n",
      "                \"RotationOrder\": \"ZXY\",\n",
      "                \"RotationRule\": \"right-hand\",\n",
      "                \"Description\": \"This global reference frame is defined by the IMU axes: X right, Y anterior, Z superior. The scene camera frame differs from this frame by a 102-degree rotation around the X-axis. All motion data are expressed relative to the IMU frame for consistency.\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "channels_json_path = motion_dir / f\"{prefix}_channels.json\"\n",
    "with open(channels_json_path, \"r\") as f:\n",
    "    channels_metadata = json.load(f)\n",
    "print(json.dumps(channels_metadata, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to Eye-Tracking-BIDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Duplicated indices found and removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qian.chu\\Documents\\GitHub\\PyNeon\\pyneon\\preprocess\\preprocess.py:67: UserWarning: 23 out of 48219 requested timestamps are outside the data time range and will have empty data.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_channels.json\n",
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_channels.tsv\n",
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_motion.json\n",
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_motion.tsv\n",
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physio.json\n",
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physio.tsv.gz\n",
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physioevents.json\n",
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physioevents.tsv\n",
      "sub-1_task-LabMuse_tracksys-NeonIMU_run-1_physioevents.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qian.chu\\Documents\\GitHub\\PyNeon\\pyneon\\export\\export_bids.py:233: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  physioevents_data = pd.concat([physioevents_data, events_data], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "rec.export_eye_bids(motion_dir, prefix=prefix)\n",
    "# Print all the conents of motion_dir\n",
    "for path in motion_dir.iterdir():\n",
    "    print(path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eye-tracking data shape: (48218, 5)\n",
      "   1758493516475391023   535.36  1016.989  4.6517  4.5655\n",
      "0  1758493516480394023  519.555  1033.236  4.0452  4.5209\n",
      "1  1758493516485390023  521.541  1045.861  3.4693  4.5801\n",
      "2  1758493516490390023  503.323  1049.000  4.1283  4.7350\n",
      "3  1758493516495406023  498.538  1065.085  4.5646  4.8635\n",
      "4  1758493516500390023  485.796  1069.292  4.7579  4.8452\n"
     ]
    }
   ],
   "source": [
    "physio_tsv_path = motion_dir / f\"{prefix}_physio.tsv.gz\"\n",
    "physio_df = pd.read_csv(physio_tsv_path, sep=\"\\t\", compression=\"gzip\")\n",
    "print(f\"Eye-tracking data shape: {physio_df.shape}\")\n",
    "print(physio_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"SamplingFrequency\": 199.71919360432685,\n",
      "    \"StartTime\": 0,\n",
      "    \"Columns\": [\n",
      "        \"timestamp\",\n",
      "        \"x_coordinate\",\n",
      "        \"y_coordinate\",\n",
      "        \"left_pupil_diameter\",\n",
      "        \"right_pupil_diameter\"\n",
      "    ],\n",
      "    \"DeviceSerialNumber\": \"114837\",\n",
      "    \"Manufacturer\": \"Pupil Labs\",\n",
      "    \"ManufacturersModelName\": \"Neon\",\n",
      "    \"SoftwareVersions\": \"App version: 2.9.26-prod; Pipeline version: 2.8.0\",\n",
      "    \"PhysioType\": \"eyetrack\",\n",
      "    \"EnvironmentCoorinates\": \"top-left\",\n",
      "    \"RecordedEye\": \"cyclopean\",\n",
      "    \"SampleCoordinateSystem\": \"gaze-in-world\",\n",
      "    \"EyeTrackingMethod\": \"real-time neural network\",\n",
      "    \"timestamp\": {\n",
      "        \"Description\": \"UTC timestamp in nanoseconds of the sample\",\n",
      "        \"Units\": \"ns\"\n",
      "    },\n",
      "    \"x_coordinate\": {\n",
      "        \"LongName\": \"Gaze position (x)\",\n",
      "        \"Description\": \"Horizontal gaze position x-coordinate in the scene camera frame, measured from the top-left corner\",\n",
      "        \"Units\": \"pixel\"\n",
      "    },\n",
      "    \"y_coordinate\": {\n",
      "        \"LongName\": \"Gaze position (y)\",\n",
      "        \"Description\": \"Vertical gaze position y-coordinate in the scene camera frame, measured from the top-left corner\",\n",
      "        \"Units\": \"pixel\"\n",
      "    },\n",
      "    \"pupil_size_left\": {\n",
      "        \"Description\": \"Physical diameter of the left eye pupil, measured in millimeters\",\n",
      "        \"Units\": \"mm\"\n",
      "    },\n",
      "    \"pupil_size_right\": {\n",
      "        \"Description\": \"Physical diameter of the right eye pupil, measured in millimeters\",\n",
      "        \"Units\": \"mm\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "physio_json = motion_dir / f\"{prefix}_physio.json\"\n",
    "with open(physio_json, \"r\") as f:\n",
    "    physio_metadata = json.load(f)\n",
    "print(json.dumps(physio_metadata, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eye-tracking data shape: (1101, 4)\n",
      "                 onset  duration trial_type          message\n",
      "0  1758493514096000000       NaN        NaN  recording.begin\n",
      "1  1758493516475391023     0.105    saccade              NaN\n",
      "2  1758493516580513023     0.535   fixation              NaN\n",
      "3  1758493517116002023     0.110    saccade              NaN\n",
      "4  1758493517226130023     0.135   fixation              NaN\n"
     ]
    }
   ],
   "source": [
    "physioevents_tsv_path = motion_dir / f\"{prefix}_physioevents.tsv.gz\"\n",
    "physioevents_df = pd.read_csv(physioevents_tsv_path, sep=\"\\t\", compression=\"gzip\")\n",
    "print(f\"Eye-tracking data shape: {physioevents_df.shape}\")\n",
    "print(physioevents_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Columns\": [\n",
      "        \"onset\",\n",
      "        \"duration\",\n",
      "        \"trial_type\",\n",
      "        \"message\"\n",
      "    ],\n",
      "    \"Description\": \"Eye events and messages logged by Neon\",\n",
      "    \"OnsetSource\": \"timestamp\",\n",
      "    \"onset\": {\n",
      "        \"Description\": \"UTC timestamp in nanoseconds of the start of the event\",\n",
      "        \"Units\": \"ns\"\n",
      "    },\n",
      "    \"duration\": {\n",
      "        \"Description\": \"Event duration in seconds\",\n",
      "        \"Units\": \"s\"\n",
      "    },\n",
      "    \"trial_type\": {\n",
      "        \"Description\": \"Type of trial event\",\n",
      "        \"Levels\": {\n",
      "            \"fixation\": {\n",
      "                \"Description\": \"Fixation event\"\n",
      "            },\n",
      "            \"saccade\": {\n",
      "                \"Description\": \"Saccade event\"\n",
      "            },\n",
      "            \"blink\": {\n",
      "                \"Description\": \"Blink event\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "physioevents_json = motion_dir / f\"{prefix}_physioevents.json\"\n",
    "with open(physioevents_json, \"r\") as f:\n",
    "    physioevents_metadata = json.load(f)\n",
    "print(json.dumps(physioevents_metadata, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
