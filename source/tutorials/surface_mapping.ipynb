{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Mapping Using AprilTags and ArUco markers\n",
    "\n",
    "## Introduction to Fiducial Markers\n",
    "\n",
    "Fiducial markers (for a review see[<sup>1</sup>](https://doi.org/10.1007/s10846-020-01307-9)) are specially designed visual markers used to establish spatial correspondence between different coordinate systems. They serve as reference points in images that can be reliably and accurately detected, allowing us to map between video/image coordinates and real-world coordinates. In eye-tracking applications, fiducial markers enable us to transform gaze coordinates from the camera's perspective to the coordinates of the observed surface (e.g., a computer screen).\n",
    "\n",
    "For surface mapping, PyNeon supports the use of two widely adopted fiducial marker systems: AprilTags and ArUco markers. Both have a barcode-like appearance and encode unique IDs in their patterns.\n",
    "\n",
    "- [AprilTag](https://april.eecs.umich.edu/software/apriltag): see references<sup>[2](https://doi.org/10.1109/ICRA.2011.5979561),[3](https://doi.org/10.1109/IROS.2016.7759617)</sup>. Pupil Labs offers AprilTag-based surface mapping in [Neon Player](https://docs.pupil-labs.com/neon/neon-player/surface-tracker/) and in [Pupil Cloud](https://docs.pupil-labs.com/neon/pupil-cloud/enrichments/marker-mapper/). However, PyNeon's implementation allows for more customizable detection parameters.\n",
    "\n",
    "- [ArUco](https://www.uco.es/investiga/grupos/ava/portfolio/aruco/): see reference<sup>[4](https://doi.org/10.1016/j.patcog.2014.01.005)</sup>. Pre-defined ArUco dictionaries for generating and detecting these makers are integrated into OpenCV. Therefore, PyNeon uses OpenCV's ArUco module ([cv2.aruco](https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html)) for marker detection (including AprilTag).\n",
    "\n",
    "<p><a href=\"https://commons.wikimedia.org/wiki/File:Comparison_of_augmented_reality_fiducial_markers.svg#/media/File:Comparison_of_augmented_reality_fiducial_markers.svg\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/Comparison_of_augmented_reality_fiducial_markers.svg\" alt=\"Comparison of augmented reality fiducial markers.svg\" width=\"600\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Sample Data\n",
    "\n",
    "To illustrate surface mapping using fiducial markers, we will use a sample dataset called \"markers\". the dataset includes two recordings: one with AprilTag markers and another with ArUco markers. In both recordings, a pilot participant viewed a set of artworks displayed on a computer screen. Our goal is the map the gaze and fixation data onto the computer screen using the fiducial markers present in the scene camera video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyneon import Dataset, get_sample_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load a sample recording\n",
    "dataset_dir = get_sample_data(\"markers\", format=\"cloud\")\n",
    "dataset = Dataset(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the AprilTag recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = dataset.recordings[0]\n",
    "print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = rec.scene_video\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 3))\n",
    "video.plot_frame(380, ax=axs[0], show=False)\n",
    "video.plot_frame(500, ax=axs[1], show=False)\n",
    "video.plot_frame(520, ax=axs[2], show=False)\n",
    "axs[0].set_title(\"Fixation\")\n",
    "axs[1].set_title(\"Image\")\n",
    "axs[2].set_title(\"ISI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of the frames shown above, we can see QR-code like markers at the borders of the screen. These are called apriltags and can be used as fiducial markers to relate video to real-world coordinates. PyNeon wraps a function that performs the detection of these. For computational efficiency, we only perform one detection every 5 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pyneon import Stream\n",
    "if Path(\"detected_markers.csv\").exists():\n",
    "    detected_markers = Stream(\"detected_markers.csv\")\n",
    "else:\n",
    "    detected_markers = video.detect_markers(\"36h11\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_markers.data.to_csv(\"detected_markers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a few frames with detected markers\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 6), tight_layout=True)\n",
    "axs = axs.flatten()\n",
    "video.plot_detected_markers(detected_markers, frame_index=500, ax=axs[0], show=False)\n",
    "video.plot_detected_markers(detected_markers, frame_index=1000, ax=axs[1], show=False)\n",
    "video.plot_detected_markers(detected_markers, frame_index=1500, ax=axs[2], show=False)\n",
    "video.plot_detected_markers(detected_markers, frame_index=2000, ax=axs[3], show=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having detected the markers (apriltags), we now need to provide information on the real-world coordinates of our markers. This is solved via a marker_info dataframe, which we generate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyneon.vis import plot_marker_layout\n",
    "import pandas as pd\n",
    "\n",
    "marker_layout = pd.DataFrame(\n",
    "    {\n",
    "        \"marker name\": [f\"36h11_{i}\" for i in range(6)],\n",
    "        \"size\": 200,\n",
    "        \"center x\": [150, 1770, 1770, 1770, 150, 150],\n",
    "        \"center y\": [150, 150, 540, 930, 930, 540],\n",
    "    }\n",
    ")\n",
    "print(marker_layout)\n",
    "plot_marker_layout(marker_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can now run the ``find_homograpghy()`` method. This method finds the map between the detections and the provided coordinates for each frame. As we did not do detections in every frame, we further provide the skip_frames as used before so that the homographies can be interpolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyneon.video import find_homographies\n",
    "\n",
    "homographies = find_homographies(\n",
    "    detected_markers,\n",
    "    marker_layout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(homographies.data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_on_surface = rec.gaze.apply_homographies(homographies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with this, we can finally transform both gaze and fixation coordinates into the screen's reference frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.scatter(\n",
    "    gaze_on_surface[\"gaze x [surface coord]\"],\n",
    "    gaze_on_surface[\"gaze y [surface coord]\"],\n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "# plt.scatter(\n",
    "#     fix_data[\"fixation x [surface coord]\"],\n",
    "#     fix_data[\"fixation y [surface coord]\"],\n",
    "#     s=1,\n",
    "#     c=\"black\",\n",
    "#     label=\"Fixations\",\n",
    "# )\n",
    "\n",
    "plt.plot(\n",
    "    [360, 1560, 1560, 360, 360],\n",
    "    [140, 140, 940, 940, 140],\n",
    "    color=\"red\",\n",
    "    label=\"Image outline\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"X Coordinate (surface coord)\")\n",
    "plt.ylabel(\"Y Coordinate (surface coord)\")\n",
    "\n",
    "plt.xlim(0, 1920)\n",
    "plt.ylim(0, 1080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArUco Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = dataset.recordings[1]\n",
    "detected_markers = rec.scene_video.detect_markers(\"5x5_100\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyneon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
