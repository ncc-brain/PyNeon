{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Mapping Using AprilTags and ArUco markers\n",
    "\n",
    "## Introduction to Fiducial Markers\n",
    "\n",
    "Fiducial markers (for a review see[<sup>1</sup>](https://doi.org/10.1007/s10846-020-01307-9)) are specially designed visual markers used to establish spatial correspondence between different coordinate systems. They serve as reference points in images that can be reliably and accurately detected, allowing us to map between video/image coordinates and real-world coordinates. In eye-tracking applications, fiducial markers enable us to transform gaze coordinates from the camera's perspective to the coordinates of the observed surface (e.g., a computer screen).\n",
    "\n",
    "For surface mapping, PyNeon supports the use of two widely adopted fiducial marker systems: AprilTags and ArUco markers. Both have a barcode-like appearance and encode unique IDs in their patterns.\n",
    "\n",
    "- [AprilTag](https://april.eecs.umich.edu/software/apriltag): see references<sup>[2](https://doi.org/10.1109/ICRA.2011.5979561),[3](https://doi.org/10.1109/IROS.2016.7759617)</sup>. Pupil Labs offers AprilTag-based surface mapping in [Neon Player](https://docs.pupil-labs.com/neon/neon-player/surface-tracker/) and in [Pupil Cloud](https://docs.pupil-labs.com/neon/pupil-cloud/enrichments/marker-mapper/). However, PyNeon's implementation allows for more customizable detection parameters.\n",
    "\n",
    "- [ArUco](https://www.uco.es/investiga/grupos/ava/portfolio/aruco/): see reference<sup>[4](https://doi.org/10.1016/j.patcog.2014.01.005)</sup>. Pre-defined ArUco dictionaries for generating and detecting these makers are integrated into OpenCV. Therefore, PyNeon uses OpenCV's ArUco module ([cv2.aruco](https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html)) for marker detection (including AprilTag).\n",
    "\n",
    "<p><a href=\"https://commons.wikimedia.org/wiki/File:Comparison_of_augmented_reality_fiducial_markers.svg#/media/File:Comparison_of_augmented_reality_fiducial_markers.svg\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/Comparison_of_augmented_reality_fiducial_markers.svg\" alt=\"Comparison of augmented reality fiducial markers.svg\" width=\"600\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Sample Data\n",
    "\n",
    "To illustrate surface mapping using fiducial markers, we will use a sample dataset called \"markers\". the dataset includes two recordings: one with AprilTag markers and another with ArUco markers. In both recordings, a pilot participant viewed a set of artworks displayed on a computer screen. Our goal is the map the gaze and fixation data onto the computer screen using the fiducial markers present in the scene camera video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'apply_homographies_on_gaze' from 'pyneon.video' (C:\\Users\\qian.chu\\Documents\\GitHub\\PyNeon\\pyneon\\video\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyneon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, get_sample_data\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load a sample recording\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\PyNeon\\pyneon\\__init__.py:8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypeguard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m install_import_hook\n\u001b[32m      6\u001b[39m install_import_hook(\u001b[33m\"\u001b[39m\u001b[33mpyneon\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mepochs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Epochs, construct_epochs_info, events_to_epochs_info\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Events\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qian.chu\\AppData\\Local\\miniforge3\\envs\\pyneon\\Lib\\site-packages\\typeguard\\_importhook.py:98\u001b[39m, in \u001b[36mTypeguardLoader.exec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexec_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module: ModuleType) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     92\u001b[39m     \u001b[38;5;66;03m# Use a custom optimization marker – the import lock should make this monkey\u001b[39;00m\n\u001b[32m     93\u001b[39m     \u001b[38;5;66;03m# patch safe\u001b[39;00m\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m patch(\n\u001b[32m     95\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimportlib._bootstrap_external.cache_from_source\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     96\u001b[39m         optimized_cache_from_source,\n\u001b[32m     97\u001b[39m     ):\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\PyNeon\\pyneon\\dataset.py:6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrecording\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Recording\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mDataset\u001b[39;00m:\n\u001b[32m     10\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m    Holder for multiple recordings. It reads from a directory containing a multiple\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03m    recordings downloaded from Pupil Cloud with the **Timeseries CSV** or\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m \n\u001b[32m     49\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qian.chu\\AppData\\Local\\miniforge3\\envs\\pyneon\\Lib\\site-packages\\typeguard\\_importhook.py:98\u001b[39m, in \u001b[36mTypeguardLoader.exec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexec_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module: ModuleType) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     92\u001b[39m     \u001b[38;5;66;03m# Use a custom optimization marker – the import lock should make this monkey\u001b[39;00m\n\u001b[32m     93\u001b[39m     \u001b[38;5;66;03m# patch safe\u001b[39;00m\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m patch(\n\u001b[32m     95\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimportlib._bootstrap_external.cache_from_source\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     96\u001b[39m         optimized_cache_from_source,\n\u001b[32m     97\u001b[39m     ):\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\PyNeon\\pyneon\\recording.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc_decorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fill_doc\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calib_dtype, expected_files_cloud, expected_files_native\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvideo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     22\u001b[39m     Video,\n\u001b[32m     23\u001b[39m     detect_markers,\n\u001b[32m     24\u001b[39m     estimate_camera_pose,\n\u001b[32m     25\u001b[39m     estimate_scanpath,\n\u001b[32m     26\u001b[39m     find_homographies,\n\u001b[32m     27\u001b[39m     apply_homographies_on_gaze,\n\u001b[32m     28\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m overlay_detections_and_pose, overlay_scanpath, plot_distribution\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRecording\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'apply_homographies_on_gaze' from 'pyneon.video' (C:\\Users\\qian.chu\\Documents\\GitHub\\PyNeon\\pyneon\\video\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from pyneon import Dataset, get_sample_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load a sample recording\n",
    "dataset_dir = get_sample_data(\"markers\", format=\"cloud\")\n",
    "dataset = Dataset(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the AprilTag recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = dataset.recordings[0]\n",
    "print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = rec.scene_video\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 3))\n",
    "video.plot_frame(380, ax=axs[0], show=False)\n",
    "video.plot_frame(500, ax=axs[1], show=False)\n",
    "video.plot_frame(520, ax=axs[2], show=False)\n",
    "axs[0].set_title(\"Fixation\")\n",
    "axs[1].set_title(\"Image\")\n",
    "axs[2].set_title(\"ISI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of the frames shown above, we can see QR-code like markers at the borders of the screen. These are called apriltags and can be used as fiducial markers to relate video to real-world coordinates. PyNeon wraps a function that performs the detection of these. For computational efficiency, we only perform one detection every 5 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_markers = video.detect_markers(\"36h11\", detection_window=(450, 530), detection_window_unit=\"frame\")\n",
    "print(detected_markers.data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only get markers detected in frame\n",
    "frame = 500\n",
    "video.plot_detected_markers(detected_markers, frame_index=frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having detected the markers (apriltags), we now need to provide information on the real-world coordinates of our markers. This is solved via a marker_info dataframe, which we generate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyneon.vis import plot_marker_layout\n",
    "import pandas as pd\n",
    "\n",
    "marker_layout = pd.DataFrame(\n",
    "    {\n",
    "        \"marker name\": [f\"36h11_{i}\" for i in range(6)],\n",
    "        \"size\": 200,\n",
    "        \"center x\": [150, 1770, 1770, 1770, 150, 150],\n",
    "        \"center y\": [150, 150, 540, 930, 930, 540],\n",
    "    }\n",
    ")\n",
    "print(marker_layout)\n",
    "plot_marker_layout(marker_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can now run the ``find_homograpghy()`` method. This method finds the map between the detections and the provided coordinates for each frame. As we did not do detections in every frame, we further provide the skip_frames as used before so that the homographies can be interpolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyneon.video import find_homographies\n",
    "\n",
    "homographies = find_homographies(\n",
    "    detected_markers,\n",
    "    marker_layout,\n",
    ")\n",
    "gaze_on_screen = rec.gaze.apply_homographies(homographies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(homographies.data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with this, we can finally transform both gaze and fixation coordinates into the screen's reference frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_data = gaze_on_screen.data\n",
    "fix_data = fixations_on_screen.data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.scatter(\n",
    "    gaze_data[\"gaze x [surface coord]\"],\n",
    "    gaze_data[\"gaze y [surface coord]\"],\n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    "    c=gaze_data.index,\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    fix_data[\"fixation x [surface coord]\"],\n",
    "    fix_data[\"fixation y [surface coord]\"],\n",
    "    s=1,\n",
    "    c=\"black\",\n",
    "    label=\"Fixations\",\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    [-600, 600, 600, -600, -600],\n",
    "    [-400, -400, 400, 400, -400],\n",
    "    color=\"red\",\n",
    "    label=\"Image outline\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"X Coordinate (surface coord)\")\n",
    "plt.ylabel(\"Y Coordinate (surface coord)\")\n",
    "\n",
    "plt.xlim(-800, 800)\n",
    "plt.ylim(-600, 600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyneon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
