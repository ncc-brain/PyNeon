{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Mapping using Apriltags\n",
    "\n",
    "In this notebook, we show how to use AprilTags to map from gaze-to screen coordinates. Along the way, we will also show some additional epoching functionalities.\n",
    "\n",
    "To do so, we have collected a dataset where a participant freely views artworks presented on screen. Each artwork is shown for 4s, followed by a 0.5s inter trial intervall and a 0.5s central fixation cross. We can download this dataset using `get_sample_data()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (recording.py, line 1164)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "  File \u001b[92mc:\\Users\\QianC\\miniforge3\\envs\\pyneon\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3701\u001b[39m in \u001b[95mrun_code\u001b[39m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  Cell \u001b[92mIn[1]\u001b[39m\u001b[92m, line 3\u001b[39m\n    from pyneon import Recording, get_sample_data\n",
      "  File \u001b[92mD:\\GitHub\\PyNeon\\pyneon\\__init__.py:8\u001b[39m\n    from .dataset import Dataset\n",
      "  File \u001b[92m<frozen importlib._bootstrap>:1360\u001b[39m in \u001b[95m_find_and_load\u001b[39m\n",
      "  File \u001b[92m<frozen importlib._bootstrap>:1331\u001b[39m in \u001b[95m_find_and_load_unlocked\u001b[39m\n",
      "  File \u001b[92m<frozen importlib._bootstrap>:935\u001b[39m in \u001b[95m_load_unlocked\u001b[39m\n",
      "  File \u001b[92mc:\\Users\\QianC\\miniforge3\\envs\\pyneon\\Lib\\site-packages\\typeguard\\_importhook.py:98\u001b[39m in \u001b[95mexec_module\u001b[39m\n    super().exec_module(module)\n",
      "  File \u001b[92mD:\\GitHub\\PyNeon\\pyneon\\dataset.py:6\u001b[39m\n    from .recording import Recording\n",
      "  File \u001b[92m<frozen importlib._bootstrap>:1360\u001b[39m in \u001b[95m_find_and_load\u001b[39m\n",
      "  File \u001b[92m<frozen importlib._bootstrap>:1331\u001b[39m in \u001b[95m_find_and_load_unlocked\u001b[39m\n",
      "  File \u001b[92m<frozen importlib._bootstrap>:935\u001b[39m in \u001b[95m_load_unlocked\u001b[39m\n",
      "  File \u001b[92mc:\\Users\\QianC\\miniforge3\\envs\\pyneon\\Lib\\site-packages\\typeguard\\_importhook.py:98\u001b[39m in \u001b[95mexec_module\u001b[39m\n    super().exec_module(module)\n",
      "  File \u001b[92m<frozen importlib._bootstrap_external>:1019\u001b[39m in \u001b[95mexec_module\u001b[39m\n",
      "  File \u001b[92m<frozen importlib._bootstrap_external>:1157\u001b[39m in \u001b[95mget_code\u001b[39m\n",
      "  File \u001b[92mc:\\Users\\QianC\\miniforge3\\envs\\pyneon\\Lib\\site-packages\\typeguard\\_importhook.py:68\u001b[39m in \u001b[95msource_to_code\u001b[39m\n    tree = _call_with_frames_removed(\n",
      "  File \u001b[92mc:\\Users\\QianC\\miniforge3\\envs\\pyneon\\Lib\\site-packages\\typeguard\\_importhook.py:47\u001b[39m in \u001b[95m_call_with_frames_removed\u001b[39m\n    return f(*args, **kwargs)\n",
      "\u001b[36m  \u001b[39m\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\QianC\\miniforge3\\envs\\pyneon\\Lib\\ast.py:50\u001b[39m\u001b[36m in \u001b[39m\u001b[35mparse\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn compile(source, filename, mode, flags,\u001b[39m\n",
      "  \u001b[36mFile \u001b[39m\u001b[32mD:\\GitHub\\PyNeon\\pyneon\\recording.py:1164\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m- 'frame index': Video frame index\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyneon import Recording, get_sample_data\n",
    "\n",
    "# Load a sample recording\n",
    "rec_dir = (\n",
    "    get_sample_data(\"Artworks\") / \"Timeseries Data + Scene Video\" / \"artworks-9a141750\"\n",
    ")\n",
    "rec = Recording(rec_dir)\n",
    "print(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a closer look at the events file, populated with triggers sent during the initial viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data = rec.events.data\n",
    "print(event_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the name column holds the information ofinterest to us. As the column ontains both information about the event_type as well as its name, we will first parse these apart. This process is a bit lengthy, but necessary to make sense of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the name column, look for the key words image onset and image offset\n",
    "def parse_event_name(name):\n",
    "    if \"image onset\" in name.lower():\n",
    "        type = \"image onset\"\n",
    "        name = name.lower().replace(\"image onset \", \"\").strip()\n",
    "    elif \"image offset\" in name.lower():\n",
    "        type = \"image offset\"\n",
    "        name = name.lower().replace(\"image offset \", \"\").strip()\n",
    "    else:\n",
    "        type = \"other\"\n",
    "        name = None\n",
    "    return type, name\n",
    "\n",
    "\n",
    "for i, row in event_data.iterrows():\n",
    "    type, name = parse_event_name(row[\"name\"])\n",
    "    event_data.at[i, \"event_type\"] = type\n",
    "    event_data.at[i, \"image_name\"] = name\n",
    "\n",
    "print(event_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "video = rec.scene_video\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 3))\n",
    "video.plot_frame(170, ax=axs[0], show=False)\n",
    "video.plot_frame(200, ax=axs[1], show=False)\n",
    "video.plot_frame(320, ax=axs[2], show=False)\n",
    "axs[0].set_title(\"Fixation\")\n",
    "axs[1].set_title(\"Image\")\n",
    "axs[2].set_title(\"ISI\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of the frames shown above, we can see QR-code like markers at the borders of the screen. These are called apriltags and can be used as fiducial markers to relate video to real-world coordinates. PyNeon wraps a function that performs the detection of these. For computational efficiency, we only perform one detection every 30 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_markers = video.detect_markers(\n",
    "    detection_window=(180, 210), detection_window_unit=\"frame\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_markers.data.loc[detected_markers.ts[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Only get markers detected in frame\u001b[39;00m\n\u001b[32m      2\u001b[39m frame = \u001b[32m210\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mvideo\u001b[49m.plot_detected_markers(detected_markers, frame_id=frame)\n",
      "\u001b[31mNameError\u001b[39m: name 'video' is not defined"
     ]
    }
   ],
   "source": [
    "# Only get markers detected in frame\n",
    "frame = 210\n",
    "video.plot_detected_markers(detected_markers, frame_index=frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having detected the markers (apriltags), we now need to provide information on the real-world coordinates of our markers. This is solved via a marker_info dataframe, which we generate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyneon.vis import plot_marker_layout\n",
    "import pandas as pd\n",
    "\n",
    "marker_layout = pd.DataFrame(\n",
    "    {\n",
    "        \"marker name\": [f\"36h11_{i}\" for i in range(6)],\n",
    "        \"size\": 200,\n",
    "        \"center x\": [100, 100, 100, 1820, 1820, 1820],\n",
    "        \"center y\": [100, 540, 980, 100, 540, 980],\n",
    "    }\n",
    ")\n",
    "\n",
    "plot_marker_layout(marker_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(marker_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can now run the ``find_homograpghy()`` method. This method finds the map between the detections and the provided coordinates for each frame. As we did not do detections in every frame, we further provide the skip_frames as used before so that the homographies can be interpolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast index to np.ndarray\n",
    "index = rec.gaze.data.index.to_numpy()\n",
    "print(index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyneon.video import find_homographies\n",
    "homographies = find_homographies(\n",
    "    marker_layout,\n",
    "    detected_markers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with this, we can finally transform both gaze and fixation coordinates into the screen's reference frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_on_screen = rec.gaze_on_surface(homographies=homographies, overwrite=True)\n",
    "fixations_on_screen = rec.fixations_on_surface(\n",
    "    gaze_on_surface=gaze_on_screen, overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_data = gaze_on_screen.data\n",
    "fix_data = fixations_on_screen.data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.scatter(\n",
    "    gaze_data[\"gaze x [surface coord]\"],\n",
    "    gaze_data[\"gaze y [surface coord]\"],\n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    "    c=gaze_data.index,\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    fix_data[\"fixation x [surface coord]\"],\n",
    "    fix_data[\"fixation y [surface coord]\"],\n",
    "    s=1,\n",
    "    c=\"black\",\n",
    "    label=\"Fixations\",\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    [-600, 600, 600, -600, -600],\n",
    "    [-400, -400, 400, 400, -400],\n",
    "    color=\"red\",\n",
    "    label=\"Image outline\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"X Coordinate (surface coord)\")\n",
    "plt.ylabel(\"Y Coordinate (surface coord)\")\n",
    "\n",
    "plt.xlim(-800, 800)\n",
    "plt.ylim(-600, 600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyneon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
