{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a Neon dataset/recording\n",
    "In this tutorial, we will show how to load a single Neon recording downloaded from [Pupil Cloud](https://docs.pupil-labs.com/neon/pupil-cloud/) and give an overview of the data structure.\n",
    "\n",
    "## Reading sample data\n",
    "We will use a sample recording produced by the NCC Lab, called `OfficeWalk`. This project (collection of recordings on Pupil Cloud) contains two recordings and multiple enrichments and can be downloaded with the `get_sample_data()` function. The function returns a `Pathlib.Path` [(reference)](https://docs.python.org/3/library/pathlib.html#pathlib.Path) object pointing to the downloaded and unzipped directory. PyNeon accepts both `Path` and `string` objects but internally always uses `Path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GitHub\\pyneon\\data\\OfficeWalk\n"
     ]
    }
   ],
   "source": [
    "from pyneon import get_sample_data, NeonDataset, NeonRecording\n",
    "\n",
    "# Download sample data (if not existing) and return the path\n",
    "sample_dir = get_sample_data(\"OfficeWalk\")\n",
    "print(sample_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `OfficeWalk` data has the following structure:\n",
    "\n",
    "```text\n",
    "OfficeWalk\n",
    "├── Timeseries Data\n",
    "│   ├── walk1-e116e606\n",
    "│   │   ├── info.json\n",
    "│   │   ├── gaze.csv\n",
    "│   │   └── ....\n",
    "│   ├── walk2-93b8c234\n",
    "│   │   ├── info.json\n",
    "│   │   ├── gaze.csv\n",
    "│   │   └── ....\n",
    "|   ├── enrichment_info.txt\n",
    "|   └── sections.csv\n",
    "├── OfficeWalk_FACE-MAPPER_FaceMap\n",
    "├── OfficeWalk_MARKER-MAPPER_TagMap_csv\n",
    "└── OfficeWalk_STATIC-IMAGE-MAPPER_ManualMap_csv\n",
    "```\n",
    "\n",
    "The `Timeseries Data` folder contains what PyNeon refers to as a `NeonDataset`. It consists of two recordings, each with its own `info.json` file and data files. These recordings can be loaded either individually as a `NeonRecording` as a collective `NeonDataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a `NeonDataset`, specify the path to the `Timeseries Data` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeonDataset | 2 recordings\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = sample_dir / \"Timeseries Data\"\n",
    "dataset = NeonDataset(dataset_dir)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeonDataset has a `recordings` attribute containing a list of `NeonRecording` objects. You can access individual recordings by index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyneon.recording.NeonRecording'>\n",
      "D:\\GitHub\\pyneon\\data\\OfficeWalk\\Timeseries Data\\walk2-93b8c234\n"
     ]
    }
   ],
   "source": [
    "first_recording = dataset[0]\n",
    "print(type(first_recording))\n",
    "print(first_recording.recording_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can directly load a single `NeonRecording` by specifying the recording's folder path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyneon.recording.NeonRecording'>\n",
      "D:\\GitHub\\pyneon\\data\\OfficeWalk\\Timeseries Data\\walk1-e116e606\n"
     ]
    }
   ],
   "source": [
    "recording_dir = dataset_dir / \"walk1-e116e606\"\n",
    "recording = NeonRecording(recording_dir)\n",
    "print(type(recording))\n",
    "print(recording.recording_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and metadata of a NeonRecording\n",
    "You can quickly get an overview of the metadata and contents of a `NeonRecording` by printing the object. The basic metadata (e.g., recording and wearer ID, recording start time and duration) and the path to available data will be displayed. At this point, the data is simply located from the recording's folder path, but it is not yet loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recording ID: e116e606-5f3f-4d34-8727-040b8762cef8\n",
      "Wearer ID: bcff2832-cfcb-4f89-abef-7bbfe91ec561\n",
      "Wearer name: Qian\n",
      "Recording start time: 2024-08-30 17:37:01.527000\n",
      "Recording duration: 98.213s\n",
      "                  exist              filename                                                                                  path\n",
      "3d_eye_states      True     3d_eye_states.csv     D:\\GitHub\\pyneon\\data\\OfficeWalk\\Timeseries Data\\walk1-e116e606\\3d_eye_states.csv\n",
      "blinks             True            blinks.csv            D:\\GitHub\\pyneon\\data\\OfficeWalk\\Timeseries Data\\walk1-e116e606\\blinks.csv\n",
      "events             True            events.csv            D:\\GitHub\\pyneon\\data\\OfficeWalk\\Timeseries Data\\walk1-e116e606\\events.csv\n",
      "fixations          True         fixations.csv         D:\\GitHub\\pyneon\\data\\OfficeWalk\\Timeseries Data\\walk1-e116e606\\fixations.csv\n",
      "gaze               True              gaze.csv              D:\\GitHub\\pyneon\\data\\OfficeWalk\\Timeseries Data\\walk1-e116e606\\gaze.csv\n",
      "imu                True               imu.csv               D:\\GitHub\\pyneon\\data\\OfficeWalk\\Timeseries Data\\walk1-e116e606\\imu.csv\n",
      "labels             True            labels.csv            D:\\GitHub\\pyneon\\data\\OfficeWalk\\Timeseries Data\\walk1-e116e606\\labels.csv\n",
      "saccades           True          saccades.csv          D:\\GitHub\\pyneon\\data\\OfficeWalk\\Timeseries Data\\walk1-e116e606\\saccades.csv\n",
      "world_timestamps   True  world_timestamps.csv  D:\\GitHub\\pyneon\\data\\OfficeWalk\\Timeseries Data\\walk1-e116e606\\world_timestamps.csv\n",
      "scene_video_info  False                  None                                                                                  None\n",
      "scene_video       False                  None                                                                                  None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(recording)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the output, this recording includes all data files except the scene video and its metadata because we downloaded only the \"Timeseries Data\" instead of \" \"Timeseries Data + Scene Video\" from Pupil Cloud. For processing video, refer to the [Neon video tutorial](video.ipynb).\n",
    "\n",
    "Individual data streams can be accessed as properties of the `NeonRecording` object. For example, the gaze data can be accessed as `recording.gaze`, and upon accessing, the tabular data is loaded into memory. On the other hand, if you try to access unavailable data like the video, it will simply return `None` and a warning message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording.gaze is <pyneon.stream.NeonGaze object at 0x000001B2167FC830>\n",
      "recording.fixations is <pyneon.events.NeonFixations object at 0x000001B236FA6DB0>\n",
      "recording.video is None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\GitHub\\pyneon\\pyneon\\recording.py:271: UserWarning: Scene video not loaded because not all video-related files (video, scene_camera.json, world_timestamps.csv) are found.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gaze and fixation data are available\n",
    "gaze = recording.gaze\n",
    "print(f\"recording.gaze is {gaze}\")\n",
    "fixations = recording.fixations\n",
    "print(f\"recording.fixations is {fixations}\")\n",
    "\n",
    "# Video is not available\n",
    "video = recording.video\n",
    "print(f\"recording.video is {video}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyNeon reads tabular CSV file into specialized classes (e.g., gaze.csv to `NeonGaze`) which all have a `data` attribute that holds the tabular data as a `pandas.DataFrame` [(reference)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). Depending on the nature of the data, such classes could be of `NeonStream` or `NeonEV` super classes. `NeonStream` contains (semi)-continuous data streams, while `NeonEV` (dubbed so to avoid confusion with the `NeonEvent` subclass that holds data from `events.csv`) contains sparse event data.\n",
    "\n",
    "The class inheritance relationship is as follows:\n",
    "\n",
    "```text\n",
    "NeonTabular\n",
    "├── NeonStream\n",
    "│   ├── NeonGaze\n",
    "│   ├── NeonEyeStates\n",
    "│   └── NeonIMU\n",
    "└── NeonEV\n",
    "    ├── NeonBlinks\n",
    "    ├── NeonSaccades\n",
    "    ├── NeonFixations\n",
    "    └── NeonEvents\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data as dataframes\n",
    "\n",
    "The essence of `NeonTabular` is the `data` attribute—a `pandas.DataFrame`. This is a common data structure in Python for handling tabular data. For example, you can print the first 5 rows of the gaze data by calling `gaze.data.head()`, and inspect the data type of each column by calling `gaze.data.dtypes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     gaze x [px]  gaze y [px]  worn  fixation id  blink id  \\\n",
      "timestamp [ns]                                                               \n",
      "1725032224852161732     1067.486      620.856     1            1      <NA>   \n",
      "1725032224857165732     1066.920      617.117     1            1      <NA>   \n",
      "1725032224862161732     1072.699      615.780     1            1      <NA>   \n",
      "1725032224867161732     1067.447      617.062     1            1      <NA>   \n",
      "1725032224872161732     1071.564      613.158     1            1      <NA>   \n",
      "\n",
      "                     azimuth [deg]  elevation [deg]  \n",
      "timestamp [ns]                                       \n",
      "1725032224852161732      16.213030        -0.748998  \n",
      "1725032224857165732      16.176285        -0.511733  \n",
      "1725032224862161732      16.546413        -0.426618  \n",
      "1725032224867161732      16.210049        -0.508251  \n",
      "1725032224872161732      16.473521        -0.260388  \n",
      "gaze x [px]        float64\n",
      "gaze y [px]        float64\n",
      "worn                 Int32\n",
      "fixation id          Int32\n",
      "blink id             Int32\n",
      "azimuth [deg]      float64\n",
      "elevation [deg]    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(gaze.data.head())\n",
    "print(gaze.data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      fixation id   end timestamp [ns]  duration [ms]  \\\n",
      "start timestamp [ns]                                                    \n",
      "1725032224852161732             1  1725032225007283732            155   \n",
      "1725032225027282732             2  1725032225282527732            255   \n",
      "1725032225347526732             3  1725032225617770732            270   \n",
      "1725032225667907732             4  1725032225798022732            130   \n",
      "1725032225833015732             5  1725032225958137732            125   \n",
      "\n",
      "                      fixation x [px]  fixation y [px]  azimuth [deg]  \\\n",
      "start timestamp [ns]                                                    \n",
      "1725032224852161732          1069.932          614.843      16.369094   \n",
      "1725032225027282732           906.439          538.107       5.878844   \n",
      "1725032225347526732           694.843          533.982      -7.781338   \n",
      "1725032225667907732           572.983          488.800     -15.679003   \n",
      "1725032225833015732           601.861          491.125     -13.813521   \n",
      "\n",
      "                      elevation [deg]  \n",
      "start timestamp [ns]                   \n",
      "1725032224852161732         -0.367312  \n",
      "1725032225027282732          4.561914  \n",
      "1725032225347526732          4.819739  \n",
      "1725032225667907732          7.636408  \n",
      "1725032225833015732          7.512433  \n",
      "fixation id             Int32\n",
      "end timestamp [ns]      Int64\n",
      "duration [ms]           Int64\n",
      "fixation x [px]       float64\n",
      "fixation y [px]       float64\n",
      "azimuth [deg]         float64\n",
      "elevation [deg]       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(fixations.data.head())\n",
    "print(fixations.data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyNeon performs the following preprocessing when reading the CSV files:\n",
    "1. Removes the redundant `section id` and `recording id` columns that are present in the raw CSVs.\n",
    "2. Sets the `timestamp [ns]` (or `start timestamp [ns]` for most event files) column as the DataFrame index.\n",
    "3. Automatically assigns appropriate data types to columns. For instance, `Int64` type is assigned to timestamps, `Int32` to event IDs (blink/fixation/saccade ID), and `float64` to float data (e.g. gaze location, pupil size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like any other `pandas.DataFrame`, you can access individual rows, columns, or subsets of the data using the standard indexing and slicing methods. For example, `gaze.data.iloc[0]` returns the first row of the gaze data, and `gaze.data['gaze x [px]']` returns the gaze x-coordinate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of gaze data:\n",
      "gaze x [px]        1067.486\n",
      "gaze y [px]         620.856\n",
      "worn                    1.0\n",
      "fixation id             1.0\n",
      "blink id               <NA>\n",
      "azimuth [deg]      16.21303\n",
      "elevation [deg]   -0.748998\n",
      "Name: 1725032224852161732, dtype: Float64\n",
      "\n",
      "All gaze x values:\n",
      "timestamp [ns]\n",
      "1725032224852161732    1067.486\n",
      "1725032224857165732    1066.920\n",
      "1725032224862161732    1072.699\n",
      "1725032224867161732    1067.447\n",
      "1725032224872161732    1071.564\n",
      "                         ...   \n",
      "1725032319717194732     800.364\n",
      "1725032319722198732     799.722\n",
      "1725032319727194732     799.901\n",
      "1725032319732194732     796.982\n",
      "1725032319737194732     797.285\n",
      "Name: gaze x [px], Length: 18769, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"First row of gaze data:\\n{gaze.data.iloc[0]}\\n\")\n",
    "print(f\"All gaze x values:\\n{gaze.data['gaze x [px]']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful attributes and methods for NeonStream and NeonEV\n",
    "On top of analyzing `data` with `pandas.DataFrame` attributes and methods, you may also use attributes and methods of the `NeonStream` and `NeonEV` instances containing the `data` to facilitate Neon-specific data analysis. For example, `NeonStream` class has a `ts` property that allows quick access of all timestamps in the data as a `numpy.ndarray` [(reference)](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html).\n",
    "\n",
    "Useful as they are, UTC timestamps in nanoseconds are usually too large for human comprehension. Often we would want to simply know what is the relative time for each data point since the stream start (which is different from the recording start). In PyNeon, this is referred to as `times` and is in seconds. You can access it as a `numpy.ndarray` by calling the `times` property.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1725032224852161732 1725032224857165732 1725032224862161732 ...\n",
      " 1725032319727194732 1725032319732194732 1725032319737194732]\n",
      "[0.0000000e+00 5.0040000e-03 1.0000000e-02 ... 9.4875033e+01 9.4880033e+01\n",
      " 9.4885033e+01]\n"
     ]
    }
   ],
   "source": [
    "print(gaze.ts)\n",
    "print(gaze.times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timestamps (UTC, in ns) and relative time (relative to the stream start, in s) are thus the two units of time that are most commonly used in PyNeon. For example, you can crop the stream by either timestamp or relative time by calling the `crop()` method. The method takes two arguments: `start` and `end`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.885033\n",
      "9.999289\n"
     ]
    }
   ],
   "source": [
    "# Last data time of the original gaze data\n",
    "print(gaze.times[-1])\n",
    "\n",
    "# Crop the gaze data to the first 10 seconds\n",
    "gaze_cropped = gaze.crop(0, 10, by=\"time\")  # Crop by time\n",
    "print(gaze_cropped.times[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other attributes and methods available for `NeonStream` and `NeonEV` classes. For a full list, refer to the [API reference](https://ncc-brain.github.io/PyNeon/reference/data.html). We will also cover some of them in the following tutorials (e.g., [interpolation and concatenation of streams](interpolate_and_concat.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data streams and events\n",
    "\n",
    "Up to this point, PyNeon simply reads and re-organizes the raw .csv files. Let's plot some samples from the `gaze` and `eye_states` streams and a saccade from the `saccades` events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saccade id                                2.0\n",
      "end timestamp [ns]      1725032225347526656.0\n",
      "duration [ms]                            65.0\n",
      "amplitude [px]                      228.36139\n",
      "amplitude [deg]                     14.676102\n",
      "mean velocity [px/s]              3685.269894\n",
      "peak velocity [px/s]              5411.775481\n",
      "Name: 1725032225282527732, dtype: Float64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m saccade \u001b[38;5;241m=\u001b[39m fixations\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(saccade)\n\u001b[1;32m---> 19\u001b[0m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxvspan\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaccade\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaccade\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend timestamp [ns]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlightgray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m ax\u001b[38;5;241m.\u001b[39mtext(\n\u001b[0;32m     21\u001b[0m     (saccade\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m+\u001b[39m saccade[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend timestamp [ns]\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;241m1050\u001b[39m,\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaccade\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m     horizontalalignment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Visualize gaze x and pupil diameter left\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\QianC\\.conda\\envs\\pyneon\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1087\u001b[0m, in \u001b[0;36mAxes.axvspan\u001b[1;34m(self, xmin, xmax, ymin, ymax, **kwargs)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;66;03m# Strip units away.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_no_units([ymin, ymax], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mymin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mymax\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 1087\u001b[0m (xmin, xmax), \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_unit_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mxmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxmax\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1089\u001b[0m p \u001b[38;5;241m=\u001b[39m mpatches\u001b[38;5;241m.\u001b[39mRectangle((xmin, ymin), xmax \u001b[38;5;241m-\u001b[39m xmin, ymax \u001b[38;5;241m-\u001b[39m ymin, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1090\u001b[0m p\u001b[38;5;241m.\u001b[39mset_transform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xaxis_transform(which\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\QianC\\.conda\\envs\\pyneon\\Lib\\site-packages\\matplotlib\\axes\\_base.py:2585\u001b[0m, in \u001b[0;36m_AxesBase._process_unit_info\u001b[1;34m(self, datasets, kwargs, convert)\u001b[0m\n\u001b[0;32m   2583\u001b[0m     \u001b[38;5;66;03m# Update from data if axis is already set but no unit is set yet.\u001b[39;00m\n\u001b[0;32m   2584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis\u001b[38;5;241m.\u001b[39mhave_units():\n\u001b[1;32m-> 2585\u001b[0m         \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis_name, axis \u001b[38;5;129;01min\u001b[39;00m axis_map\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2587\u001b[0m     \u001b[38;5;66;03m# Return if no axis is set.\u001b[39;00m\n\u001b[0;32m   2588\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\QianC\\.conda\\envs\\pyneon\\Lib\\site-packages\\matplotlib\\axis.py:1756\u001b[0m, in \u001b[0;36mAxis.update_units\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1754\u001b[0m neednew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverter \u001b[38;5;241m!=\u001b[39m converter\n\u001b[0;32m   1755\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverter \u001b[38;5;241m=\u001b[39m converter\n\u001b[1;32m-> 1756\u001b[0m default \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_units(default)\n",
      "File \u001b[1;32mc:\\Users\\QianC\\.conda\\envs\\pyneon\\Lib\\site-packages\\matplotlib\\category.py:105\u001b[0m, in \u001b[0;36mStrCategoryConverter.default_units\u001b[1;34m(data, axis)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# the conversion call stack is default_units -> axis_info -> convert\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis\u001b[38;5;241m.\u001b[39munits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     axis\u001b[38;5;241m.\u001b[39mset_units(\u001b[43mUnitData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     axis\u001b[38;5;241m.\u001b[39munits\u001b[38;5;241m.\u001b[39mupdate(data)\n",
      "File \u001b[1;32mc:\\Users\\QianC\\.conda\\envs\\pyneon\\Lib\\site-packages\\matplotlib\\category.py:181\u001b[0m, in \u001b[0;36mUnitData.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counter \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\QianC\\.conda\\envs\\pyneon\\Lib\\site-packages\\matplotlib\\category.py:214\u001b[0m, in \u001b[0;36mUnitData.update\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# check if convertible to number:\u001b[39;00m\n\u001b[0;32m    213\u001b[0m convertible \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[43mOrderedDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# OrderedDict just iterates over unique values in data.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     _api\u001b[38;5;241m.\u001b[39mcheck_isinstance((\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m), value\u001b[38;5;241m=\u001b[39mval)\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convertible:\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;66;03m# this will only be called so long as convertible is True.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "gaze_color = \"royalblue\"\n",
    "gyro_color = \"darkorange\"\n",
    "\n",
    "imu = recording.imu\n",
    "fixations = recording.saccades\n",
    "\n",
    "# Create a figure\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax2 = ax.twinx()\n",
    "ax.yaxis.label.set_color(gaze_color)\n",
    "ax2.yaxis.label.set_color(gyro_color)\n",
    "\n",
    "# Visualize the 2nd saccade\n",
    "saccade = fixations.data.iloc[1]\n",
    "print(saccade)\n",
    "ax.axvspan(saccade.index.values, saccade[\"end timestamp [ns]\"], color=\"lightgray\")\n",
    "ax.text(\n",
    "    (saccade.index.values + saccade[\"end timestamp [ns]\"]) / 2,\n",
    "    1050,\n",
    "    \"Saccade\",\n",
    "    horizontalalignment=\"center\",\n",
    ")\n",
    "\n",
    "# Visualize gaze x and pupil diameter left\n",
    "sns.scatterplot(\n",
    "    ax=ax,\n",
    "    data=gaze.data.head(100),\n",
    "    x=gaze.data.index,\n",
    "    y=\"gaze x [px]\",\n",
    "    color=gaze_color,\n",
    ")\n",
    "sns.scatterplot(\n",
    "    ax=ax2,\n",
    "    data=imu.data.head(60),\n",
    "    x=imu.data.index,\n",
    "    y=\"gyro x [deg/s]\",\n",
    "    color=gyro_color,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's apparent that at the beginning of the recording, there are some missing data points in both the `gaze` and `imu` streams. This is presumably due to the time it takes for the sensors to start up and stabilize. We will show how to handle missing data using resampling in the next tutorial. For now, it's important to be aware of these gaps and that it will require great caution to assume the data is continuously and equally sampled.\n",
    "\n",
    "PyNeon also calculates the effective (as opposed to the nominal) sampling frequency of each stream by dividing the number of samples by the duration of the recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Gaze: nominal sampling frequency = {gaze.sampling_freq_nominal}, \"\n",
    "    f\"effective sampling frequency = {gaze.sampling_freq_effective}\"\n",
    ")\n",
    "print(\n",
    "    f\"IMU: nominal sampling frequency = {recording.imu.sampling_freq_nominal}, \"\n",
    "    f\"effective sampling frequency = {recording.imu.sampling_freq_effective}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing gaze heatmap\n",
    "Finally, we will show how to plot a heatmap of the gaze/fixation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording.plot_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can neatly see that the recorded data shows a centre-bias, which is a well-known effect from eye statistics. In y, we can see that fixations tend to occur below the horizon, which is indicative of a walking task where a participant looks at the floor in front of them more often"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyneon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
